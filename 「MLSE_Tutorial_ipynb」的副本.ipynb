{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "「MLSE Tutorial.ipynb」的副本",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JamesChung821/python/blob/master/%E3%80%8CMLSE_Tutorial_ipynb%E3%80%8D%E7%9A%84%E5%89%AF%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PR4n07wp8BEq"
      },
      "source": [
        "## MLUMD Colombia Tutorial- Information Extraction from Scientific Text\n",
        "Today, we will be using named entity recognition models and text mining to extract information on the synthesis of materials. \n",
        "\n",
        "This notebook is based on Kim, Edward, et al. \"Inorganic materials synthesis planning with literature-trained neural networks.\" Journal of Chemical Information and Modeling 60.3 (2020): 1194-1201. \n",
        "doi.org/10.1021/acs.jcim.9b00995\n",
        "\n",
        "More information and resources are available at synthesisproject.org.\n",
        "Feel free to reach out to me at zjensen@mit.edu with questions or collaboration ideas. \n",
        "\n",
        "Please make a copy of this notebook and run the code from the copy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9hDDcqtvfyM"
      },
      "source": [
        "# Extraction Workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaFAPdN-vyPh"
      },
      "source": [
        "** Focus of tutorial\n",
        "1. Obtain literature corpus relevant to your area of study.\n",
        "    - Search engines such as Scopus, Engineering Village, Crossref, Web of Science, etc.\n",
        "    - Text and data mining agreements with publishers.\n",
        "    - We have used this pipeline on corpora ranging from hundreds to hundreds of thousands of articles.\n",
        "2. Parse and clean articles\n",
        "    - We only use HTML and XML formats (PDFs are very difficult)\n",
        "    - Published repository parses for many of the popular publishers (https://github.com/CederGroupHub/LimeSoup)\n",
        "3. Classify paragraph type to determine relevant sections\n",
        "    - Introduction, synthesis, characterization, results, conclusion, etc.\n",
        "    - Use a hybrid rule-based/classifier approach\n",
        "    - Model is available at https://github.com/olivettigroup/materials-synthesis-generative-models.git\n",
        "4. **Named Entity Recognition (NER) and text mining to extract interesting entities\n",
        "    - We care about synthesis information (targets, precursors, operations, etc)\n",
        "    - Models are available at https://github.com/olivettigroup/materials-synthesis-generative-models.git\n",
        "5. **Associating Entities \n",
        "    - For example- precursors with their target material\n",
        "    - Many techniques, some include proximity-based and dependency parsing\n",
        "6. **Data Mining and Machine Learning\n",
        "    - Visualize trends in the synthesis data\n",
        "    - Use machine learning models on the extracted data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y82YxMiZ8W_A"
      },
      "source": [
        "# Imports:\n",
        "Load all the necessary libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nv_bSM7Bv-r"
      },
      "source": [
        "We need to download and install the bilm library from Github. To have the code work on CoLab, we need to first run the three cells below, then restart the runtime instance so it is recognized as an installed package. https://stackoverflow.com/questions/57838013/modulenotfounderror-after-successful-pip-install-in-google-colaboratory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnNOCOtNBm7Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ee58fbd-a74e-4a78-852d-32bcab1f2772"
      },
      "source": [
        "!git clone https://github.com/allenai/bilm-tf.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'bilm-tf'...\n",
            "remote: Enumerating objects: 292, done.\u001b[K\n",
            "remote: Total 292 (delta 0), reused 0 (delta 0), pack-reused 292\u001b[K\n",
            "Receiving objects: 100% (292/292), 588.40 KiB | 22.63 MiB/s, done.\n",
            "Resolving deltas: 100% (137/137), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsCfQu7IBlyh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "986aaa2d-6d92-4b0d-8709-b0ab140623ec"
      },
      "source": [
        "%cd bilm-tf/\n",
        "!python setup.py install"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/bilm-tf\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating bilm.egg-info\n",
            "writing bilm.egg-info/PKG-INFO\n",
            "writing dependency_links to bilm.egg-info/dependency_links.txt\n",
            "writing requirements to bilm.egg-info/requires.txt\n",
            "writing top-level names to bilm.egg-info/top_level.txt\n",
            "writing manifest file 'bilm.egg-info/SOURCES.txt'\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'bilm.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/bilm\n",
            "copying bilm/data.py -> build/lib/bilm\n",
            "copying bilm/model.py -> build/lib/bilm\n",
            "copying bilm/__init__.py -> build/lib/bilm\n",
            "copying bilm/training.py -> build/lib/bilm\n",
            "copying bilm/elmo.py -> build/lib/bilm\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/bilm\n",
            "copying build/lib/bilm/data.py -> build/bdist.linux-x86_64/egg/bilm\n",
            "copying build/lib/bilm/model.py -> build/bdist.linux-x86_64/egg/bilm\n",
            "copying build/lib/bilm/__init__.py -> build/bdist.linux-x86_64/egg/bilm\n",
            "copying build/lib/bilm/training.py -> build/bdist.linux-x86_64/egg/bilm\n",
            "copying build/lib/bilm/elmo.py -> build/bdist.linux-x86_64/egg/bilm\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bilm/data.py to data.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bilm/model.py to model.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bilm/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bilm/training.py to training.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bilm/elmo.py to elmo.cpython-37.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying bilm.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying bilm.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying bilm.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying bilm.egg-info/not-zip-safe -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying bilm.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying bilm.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "creating dist\n",
            "creating 'dist/bilm-0.1.post5-py3.7.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing bilm-0.1.post5-py3.7.egg\n",
            "creating /usr/local/lib/python3.7/dist-packages/bilm-0.1.post5-py3.7.egg\n",
            "Extracting bilm-0.1.post5-py3.7.egg to /usr/local/lib/python3.7/dist-packages\n",
            "Adding bilm 0.1.post5 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.7/dist-packages/bilm-0.1.post5-py3.7.egg\n",
            "Processing dependencies for bilm==0.1.post5\n",
            "Searching for h5py==3.1.0\n",
            "Best match: h5py 3.1.0\n",
            "Adding h5py 3.1.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for numpy==1.19.5\n",
            "Best match: numpy 1.19.5\n",
            "Adding numpy 1.19.5 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "Installing f2py3 script to /usr/local/bin\n",
            "Installing f2py3.7 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for cached-property==1.5.2\n",
            "Best match: cached-property 1.5.2\n",
            "Adding cached-property 1.5.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Finished processing dependencies for bilm==0.1.post5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "id": "03wZlkmOh4Sm",
        "outputId": "f03a5a9d-e941-4667-c4b9-25c3df5241e5"
      },
      "source": [
        "%pip install 'h5py==2.10.0' --force-reinstall"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting h5py==2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[?25l\r\u001b[K     |▏                               | 10 kB 31.4 MB/s eta 0:00:01\r\u001b[K     |▎                               | 20 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |▍                               | 30 kB 35.6 MB/s eta 0:00:01\r\u001b[K     |▌                               | 40 kB 38.1 MB/s eta 0:00:01\r\u001b[K     |▋                               | 51 kB 40.8 MB/s eta 0:00:01\r\u001b[K     |▊                               | 61 kB 43.6 MB/s eta 0:00:01\r\u001b[K     |▉                               | 71 kB 28.3 MB/s eta 0:00:01\r\u001b[K     |█                               | 81 kB 30.0 MB/s eta 0:00:01\r\u001b[K     |█                               | 92 kB 31.9 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 102 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 112 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 122 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 133 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 143 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 153 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 163 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |██                              | 174 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |██                              | 184 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 194 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 204 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 215 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 225 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 235 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 245 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 256 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |███                             | 266 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |███                             | 276 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 286 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 296 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 307 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 317 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 327 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 337 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 348 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |████                            | 358 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |████                            | 368 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 378 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 389 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 399 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 409 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 419 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 430 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████                           | 440 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████                           | 450 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 460 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 471 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 481 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 491 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 501 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 512 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 522 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████                          | 532 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████                          | 542 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 552 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 563 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 573 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 583 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 593 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 604 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 614 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████                         | 624 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████                         | 634 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 645 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 655 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 665 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 675 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 686 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 696 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 706 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████                        | 716 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████                        | 727 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 737 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 747 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 757 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 768 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 778 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 788 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 798 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 808 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 819 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 829 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 839 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 849 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 860 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 870 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 880 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 890 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 901 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 911 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 921 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 931 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 942 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 952 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 962 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 972 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 983 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 993 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 1.0 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 1.0 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 1.0 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 1.0 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 1.0 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 1.1 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 1.1 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 1.1 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 1.1 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 1.1 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 1.1 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 1.1 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 1.1 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 1.1 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 1.1 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 1.2 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 1.2 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 1.2 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 1.2 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 1.2 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 1.2 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 1.2 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 1.2 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 1.2 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 1.2 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 1.3 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 1.3 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 1.3 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 1.3 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 1.3 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 1.3 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 1.3 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 1.3 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 1.3 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 1.4 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 1.4 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 1.4 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 1.4 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 1.4 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 1.4 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 1.4 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 1.4 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.4 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.4 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 1.5 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 1.5 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 1.5 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 1.5 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 1.5 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 1.5 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.5 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.5 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 1.5 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 1.5 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 1.6 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 1.6 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 1.6 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 1.6 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 1.6 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.6 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.6 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 1.6 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 1.6 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 1.6 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 1.7 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 1.7 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 1.7 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 1.7 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.7 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.7 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 1.7 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 1.7 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.7 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 1.8 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.8 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 1.8 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.8 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.8 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.8 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.8 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 1.8 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.8 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.8 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.9 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.9 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.9 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.9 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.9 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.9 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.9 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.9 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.9 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.9 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 2.0 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 2.0 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 2.0 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 2.0 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 2.0 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 2.0 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 2.0 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 2.0 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 2.0 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 2.0 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 2.1 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 2.1 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 2.1 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 2.1 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 2.1 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 2.1 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 2.1 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 2.1 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 2.1 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 2.2 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 2.2 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 2.2 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 2.2 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 2.2 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 2.2 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 2.2 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 2.2 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 2.2 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 2.2 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 2.3 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 2.3 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 2.3 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 2.3 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 2.3 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 2.3 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 2.3 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 2.3 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 2.3 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 2.3 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 2.4 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 2.4 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 2.4 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 2.4 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 2.4 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 2.4 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 2.4 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 2.4 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 2.4 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 2.4 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 2.5 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 2.5 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 2.5 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 2.5 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 2.5 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 2.5 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 2.5 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 2.5 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 2.5 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 2.5 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 2.6 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 2.6 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 2.6 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 2.6 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 2.6 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 2.6 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 2.6 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 2.6 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 2.6 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 2.7 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 2.7 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 2.7 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 2.7 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 2.7 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 2.7 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 2.7 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 2.7 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 2.7 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 2.7 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 2.8 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 2.8 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 2.8 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 2.8 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 2.8 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 2.8 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 2.8 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 2.8 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 2.8 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 2.8 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 2.9 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.9 MB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.9 MB 31.0 MB/s \n",
            "\u001b[?25hCollecting six\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting numpy>=1.7\n",
            "  Downloading numpy-1.21.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 74 kB/s \n",
            "\u001b[?25hInstalling collected packages: six, numpy, h5py\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.5.0 requires h5py~=3.1.0, but you have h5py 2.10.0 which is incompatible.\n",
            "tensorflow 2.5.0 requires numpy~=1.19.2, but you have numpy 1.21.1 which is incompatible.\n",
            "tensorflow 2.5.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed h5py-2.10.0 numpy-1.21.1 six-1.16.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "six"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlulOV_wBx7E"
      },
      "source": [
        "------------------------------------------------------------------\n",
        "\n",
        "***We need to restart the runtime. This is so the bilm library can be loaded easily in the environment. \n",
        "Select Runtime --> Restart runtime. Then go to next cell. \n",
        "\n",
        "------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0NGKoYlevQy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d00d7ed-95d5-424e-e07a-c2465045946d"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import json\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import logging\n",
        "logging.getLogger('tensorflow').disabled = True #OPTIONAL - to disable outputs from Tensorflow"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lo_bFRKU8fy4"
      },
      "source": [
        "# Get the data\n",
        "We need to get the pretrained Elmo embedding model as well as the public Github NER repository"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mBKYxLWwhWr"
      },
      "source": [
        "NER Repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYqZT0_I0sK5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75d39383-422c-4854-e476-b9ea647c3928"
      },
      "source": [
        "!git clone https://github.com/olivettigroup/materials-synthesis-generative-models.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'materials-synthesis-generative-models'...\n",
            "remote: Enumerating objects: 678, done.\u001b[K\n",
            "remote: Counting objects: 100% (177/177), done.\u001b[K\n",
            "remote: Compressing objects: 100% (125/125), done.\u001b[K\n",
            "remote: Total 678 (delta 125), reused 95 (delta 52), pack-reused 501\u001b[K\n",
            "Receiving objects: 100% (678/678), 1.12 MiB | 15.13 MiB/s, done.\n",
            "Resolving deltas: 100% (141/141), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPH3Jxf10u2u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "430ff7f7-0e5d-4ef9-e7d6-1cdaa657cf71"
      },
      "source": [
        "%cd materials-synthesis-generative-models/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/bilm-tf/materials-synthesis-generative-models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_gKxHn9VOlq"
      },
      "source": [
        "ELMO Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nI4p-jFO7YvK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b63cc74-2289-46c9-bcc3-19b353152e3b"
      },
      "source": [
        "# Get the vocab file for Elmo, we use the default \n",
        "!wget https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/vocab-2016-09-10.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-29 16:59:03--  https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/vocab-2016-09-10.txt\n",
            "Resolving s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)... 52.218.224.128\n",
            "Connecting to s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)|52.218.224.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7430437 (7.1M) [text/plain]\n",
            "Saving to: ‘vocab-2016-09-10.txt’\n",
            "\n",
            "vocab-2016-09-10.tx 100%[===================>]   7.09M  9.52MB/s    in 0.7s    \n",
            "\n",
            "2021-07-29 16:59:04 (9.52 MB/s) - ‘vocab-2016-09-10.txt’ saved [7430437/7430437]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1ozzVCp3mmA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02b79ed3-89e4-4986-831a-96b214cb964d"
      },
      "source": [
        "# Download the pretrained-Elmo weights and config file from https://figshare.com/s/ec677e7db3cf2b7db4bf\n",
        "!wget https://ndownloader.figshare.com/files/13773791?private_link=ec677e7db3cf2b7db4bf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-29 16:59:04--  https://ndownloader.figshare.com/files/13773791?private_link=ec677e7db3cf2b7db4bf\n",
            "Resolving ndownloader.figshare.com (ndownloader.figshare.com)... 52.16.102.173, 54.217.124.219, 2a05:d018:1f4:d000:b283:27aa:b939:8ed4, ...\n",
            "Connecting to ndownloader.figshare.com (ndownloader.figshare.com)|52.16.102.173|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/13773791/elmo_finetuned_matsci.tar.gz?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=8791f5b142ba7b293ce5993c638216cfd76d10469418f29ca1f3dc5e11b425c6&X-Amz-Date=20210729T165905Z&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20210729/eu-west-1/s3/aws4_request [following]\n",
            "--2021-07-29 16:59:05--  https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/13773791/elmo_finetuned_matsci.tar.gz?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=8791f5b142ba7b293ce5993c638216cfd76d10469418f29ca1f3dc5e11b425c6&X-Amz-Date=20210729T165905Z&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20210729/eu-west-1/s3/aws4_request\n",
            "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.96.178\n",
            "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.96.178|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 347214724 (331M) [application/gzip]\n",
            "Saving to: ‘13773791?private_link=ec677e7db3cf2b7db4bf’\n",
            "\n",
            "13773791?private_li 100%[===================>] 331.13M  34.3MB/s    in 10s     \n",
            "\n",
            "2021-07-29 16:59:15 (32.0 MB/s) - ‘13773791?private_link=ec677e7db3cf2b7db4bf’ saved [347214724/347214724]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fO8N6a9kzcK-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "289af367-4094-4029-a378-a47b7b2a3433"
      },
      "source": [
        "# Unzip the weights and config file\n",
        "!tar -xvf '13773791?private_link=ec677e7db3cf2b7db4bf'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "elmo_options.json\n",
            "elmo_weights.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63zt3g6vVUwS"
      },
      "source": [
        "NER Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5wm-cxK1CJx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "834f7b13-7235-43a6-fabc-40a5da34a380"
      },
      "source": [
        "!wget https://ndownloader.figshare.com/files/25506038"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-29 16:59:20--  https://ndownloader.figshare.com/files/25506038\n",
            "Resolving ndownloader.figshare.com (ndownloader.figshare.com)... 54.217.124.219, 52.16.102.173, 2a05:d018:1f4:d000:b283:27aa:b939:8ed4, ...\n",
            "Connecting to ndownloader.figshare.com (ndownloader.figshare.com)|54.217.124.219|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/25506038/token_classifier_elmo.model?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=550edef9e315603cc4f3e79f01f8fba81a75ba633a520a74bcb67b113f1bf754&X-Amz-Date=20210729T165921Z&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20210729/eu-west-1/s3/aws4_request [following]\n",
            "--2021-07-29 16:59:21--  https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/25506038/token_classifier_elmo.model?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=550edef9e315603cc4f3e79f01f8fba81a75ba633a520a74bcb67b113f1bf754&X-Amz-Date=20210729T165921Z&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20210729/eu-west-1/s3/aws4_request\n",
            "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.0.3\n",
            "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.0.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 671409344 (640M) [application/octet-stream]\n",
            "Saving to: ‘25506038’\n",
            "\n",
            "25506038            100%[===================>] 640.31M  34.6MB/s    in 19s     \n",
            "\n",
            "2021-07-29 16:59:40 (33.2 MB/s) - ‘25506038’ saved [671409344/671409344]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHEMq3TqVXyz"
      },
      "source": [
        "ELMO Featurized Annotations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZCA_iXid3Et",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19563a3b-48ea-47d6-dc19-579b42bb1165"
      },
      "source": [
        "!wget https://ndownloader.figshare.com/files/25636313\n",
        "!wget https://ndownloader.figshare.com/files/25636334\n",
        "!wget https://ndownloader.figshare.com/files/25636421"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-29 16:59:40--  https://ndownloader.figshare.com/files/25636313\n",
            "Resolving ndownloader.figshare.com (ndownloader.figshare.com)... 54.217.124.219, 52.16.102.173, 2a05:d018:1f4:d003:1c8b:1823:acce:812, ...\n",
            "Connecting to ndownloader.figshare.com (ndownloader.figshare.com)|54.217.124.219|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/25636313/dev_elmo.p?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=75bc4cf38ab2cccf395b929aa4164cdb73a500b306eb5ed7b70faf398bfbf7a4&X-Amz-Date=20210729T165941Z&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20210729/eu-west-1/s3/aws4_request [following]\n",
            "--2021-07-29 16:59:41--  https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/25636313/dev_elmo.p?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=75bc4cf38ab2cccf395b929aa4164cdb73a500b306eb5ed7b70faf398bfbf7a4&X-Amz-Date=20210729T165941Z&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20210729/eu-west-1/s3/aws4_request\n",
            "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.100.171\n",
            "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.100.171|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 120422564 (115M) [text/x-pascal]\n",
            "Saving to: ‘25636313’\n",
            "\n",
            "25636313            100%[===================>] 114.84M  31.7MB/s    in 4.1s    \n",
            "\n",
            "2021-07-29 16:59:45 (27.9 MB/s) - ‘25636313’ saved [120422564/120422564]\n",
            "\n",
            "--2021-07-29 16:59:45--  https://ndownloader.figshare.com/files/25636334\n",
            "Resolving ndownloader.figshare.com (ndownloader.figshare.com)... 52.16.102.173, 54.217.124.219, 2a05:d018:1f4:d000:b283:27aa:b939:8ed4, ...\n",
            "Connecting to ndownloader.figshare.com (ndownloader.figshare.com)|52.16.102.173|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/25636334/test_elmo.p?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=1d34783e3d2df9d0f84a5aa7c3e851ed96a85ce6f17aed129be69209126ad4d2&X-Amz-Date=20210729T165946Z&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20210729/eu-west-1/s3/aws4_request [following]\n",
            "--2021-07-29 16:59:46--  https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/25636334/test_elmo.p?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=1d34783e3d2df9d0f84a5aa7c3e851ed96a85ce6f17aed129be69209126ad4d2&X-Amz-Date=20210729T165946Z&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20210729/eu-west-1/s3/aws4_request\n",
            "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.21.218\n",
            "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.21.218|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 123699364 (118M) [text/x-pascal]\n",
            "Saving to: ‘25636334’\n",
            "\n",
            "25636334            100%[===================>] 117.97M  32.1MB/s    in 4.2s    \n",
            "\n",
            "2021-07-29 16:59:51 (27.8 MB/s) - ‘25636334’ saved [123699364/123699364]\n",
            "\n",
            "--2021-07-29 16:59:51--  https://ndownloader.figshare.com/files/25636421\n",
            "Resolving ndownloader.figshare.com (ndownloader.figshare.com)... 54.217.124.219, 52.16.102.173, 2a05:d018:1f4:d000:b283:27aa:b939:8ed4, ...\n",
            "Connecting to ndownloader.figshare.com (ndownloader.figshare.com)|54.217.124.219|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/25636421/train_elmo.p?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=e54a276d1900af6667e36e554bcbdc30b6ff1a98e2612c0710735232f7c36c97&X-Amz-Date=20210729T165951Z&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20210729/eu-west-1/s3/aws4_request [following]\n",
            "--2021-07-29 16:59:51--  https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/25636421/train_elmo.p?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=e54a276d1900af6667e36e554bcbdc30b6ff1a98e2612c0710735232f7c36c97&X-Amz-Date=20210729T165951Z&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20210729/eu-west-1/s3/aws4_request\n",
            "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.97.202\n",
            "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.97.202|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 546406564 (521M) [text/x-pascal]\n",
            "Saving to: ‘25636421’\n",
            "\n",
            "25636421            100%[===================>] 521.09M  33.3MB/s    in 16s     \n",
            "\n",
            "2021-07-29 17:00:08 (31.9 MB/s) - ‘25636421’ saved [546406564/546406564]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mH3yWi9WeRiN"
      },
      "source": [
        "!mv 25506038 ner.model\n",
        "!mv 25636313 dev_elmo.p\n",
        "!mv 25636334 test_elmo.p\n",
        "!mv 25636421 train_elmo.p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HeE-su7pTp8"
      },
      "source": [
        "# Load the NER model and make predictions\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EYEteRQpc7r"
      },
      "source": [
        "Import code from Git repository. This repository will do all the heavy ML lifting for us interacting with Keras and Tensorflow behind the scenes.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNYNggqz33E_"
      },
      "source": [
        "from models import token_classifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71lOFdNYpjQi"
      },
      "source": [
        "Make a NER classifier instance using the ELMO files we downloaded. We are using CPU and pre-featurized data for the sake of the tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59EJG8izFn74"
      },
      "source": [
        "token_classifier = token_classifier.TokenClassifier(\n",
        "    vocab=\"vocab-2016-09-10.txt\", \n",
        "    options=\"elmo_options.json\", \n",
        "    weights=\"elmo_weights.hdf5\",\n",
        "    use_cpu=True, load_data=False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dGi58OwpwO0"
      },
      "source": [
        "Load the pre-trained NER model we downloaded earlier. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXljEceE1Isw"
      },
      "source": [
        "token_classifier.load('ner.model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxN8I5SJGK2B"
      },
      "source": [
        "Visualize the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsTruv8_GJZj"
      },
      "source": [
        "token_classifier.model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wNeQrZzrQsT"
      },
      "source": [
        "Load our pre-featurized ELMO data. For this tutorial, we are using the test set from training the model as our data set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4ZCkF8h2AmM"
      },
      "source": [
        "token_classifier.X_test = pickle.load(open('test_elmo.p', 'rb'))\n",
        "print('Featurized Data Shape (should be (302,100,1024)):', token_classifier.X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yy9G-HdRrutS"
      },
      "source": [
        "Load the text data from the repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuhdXYzkolqL"
      },
      "source": [
        "with open('data/ner_annotations_split.json', 'r') as f:\n",
        "  data = json.load(f)\n",
        "print('Data Type:', type(data))\n",
        "print('Data Keys:', data.keys())\n",
        "print('Number of Papers:', data['total_annotation_files'])\n",
        "print('Data Type for each paper:', type(data['data'][0]))\n",
        "print('Keys for each paper:', data['data'][0].keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8GvQ72uuAoa"
      },
      "source": [
        "Get all the test sentences from the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPeB1167osxl"
      },
      "source": [
        "test_sentences = []\n",
        "for paper in data['data']:\n",
        "  if paper['split'] == 'test':\n",
        "    for tokens in paper['tokens'][1:]:\n",
        "      test_sentences.append(tokens)\n",
        "print('Number of Sentences in data set:', len(test_sentences))\n",
        "print('Example Sentence', test_sentences[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MEYato2qneN"
      },
      "source": [
        "Example sentences. If your data is text than we need to convert to ELMO using \"featurize_elmo_list\". We won't be doing that for time reasons beyond this example. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ukX4N_PfBTP"
      },
      "source": [
        "example_sentences = [\n",
        "    \"The BaCO3 and TiO2 were mixed to make BaTiO3 .\".split(),\n",
        "    \"The SiO2 was heated at 700 degC .\".split()\n",
        "]\n",
        "feature_matrix = token_classifier.featurize_elmo_list(example_sentences)\n",
        "print(\"ELMO Features shape (should be (2,100,1024)):\", feature_matrix.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kT3Z7BY3DSJ6"
      },
      "source": [
        "raw_predictions = token_classifier.model.predict(feature_matrix)\n",
        "print('Example Sentences and Predictions:')\n",
        "for sentence, predictions in zip(example_sentences, raw_predictions):\n",
        "  print('---')\n",
        "  for word, prediction in zip(sentence, predictions):\n",
        "    print('  ', word, token_classifier.token_classes[np.argmax(prediction)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5V14oE17sBko"
      },
      "source": [
        "Get the raw predictions (class probabilities) using Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQwhWsYpjoCV"
      },
      "source": [
        "raw_test_predictions = token_classifier.model.predict(token_classifier.X_test)\n",
        "print('Raw Prediction Shape (should be (302, 100, 4)):', raw_test_predictions.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Axl9oZbTsITL"
      },
      "source": [
        "Format the predictions taking the most likely class as the prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ww35Z76rsMy3"
      },
      "source": [
        "test_predictions = []\n",
        "for predictions in raw_test_predictions:\n",
        "  curr_predictions = []\n",
        "  for prediction in predictions:\n",
        "    curr_predictions.append(token_classifier.token_classes[np.argmax(prediction)])\n",
        "  test_predictions.append(curr_predictions)\n",
        "print('Number of Predictions (sentences) in data set:', len(test_predictions))\n",
        "print('Example Predictions:', test_predictions[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmRIT6Zsrnnn"
      },
      "source": [
        "# Text and Data Mining From Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQKcb2oSuK7B"
      },
      "source": [
        "## Tally up targets, precursors, and operations\n",
        "For the first data mining example, we are going to look at the most common targets, precursors, and operations in the data set. This gives us a sense of what type of information our set contains, whether we have the correct data set for our task, and what types of noise we expect in the data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOBJG3me3IaU"
      },
      "source": [
        "all_targets, all_precursors, all_operations = [],[],[]\n",
        "for sentence, labels in zip(test_sentences, test_predictions):\n",
        "  prev_label = ''\n",
        "  for sent, label in zip(sentence, labels):\n",
        "    if label == 'target':\n",
        "      if prev_label == 'target':\n",
        "        all_targets[-1] = all_targets[-1]+' '+sent  # We need to combine multiword targets to help with tokenization noise effects\n",
        "      else:\n",
        "        all_targets.append(sent)\n",
        "    elif label == 'precursor':\n",
        "      if prev_label == 'precursor':\n",
        "        all_precursors[-1] = all_precursors[-1]+' '+sent\n",
        "      else:\n",
        "        all_precursors.append(sent)\n",
        "    elif label == 'operation':\n",
        "      if prev_label == 'operation':\n",
        "        all_operations[-1] = all_operations[-1]+' '+sent\n",
        "      else:\n",
        "        all_operations.append(sent)\n",
        "    prev_label = label\n",
        "print('Total Number of Targets:',  len(all_targets))\n",
        "print('Total Number of Precursors:', len(all_precursors))\n",
        "print('Total Number of Operations:', len(all_operations))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cCu8sKgvckm"
      },
      "source": [
        "We will make a simple visualization showing the ten most common targets, precursors, and operations across the entire data set. Since this is a small, randomly selected data set, we do not see overly common targets or precursors. It is also hard to see any connections between targets and precursors since the materials domains vary greatly.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WXKpjWH6FOm"
      },
      "source": [
        "fig, ax = plt.subplots(1,3, figsize=(18,6))\n",
        "plt.subplots_adjust(wspace=.55)\n",
        "ax[0].barh(np.arange(len(all_targets[:10])), pd.Series(all_targets).value_counts().values.tolist()[:10])\n",
        "ax[0].invert_yaxis()\n",
        "ax[0].set_yticks(np.arange(len(all_targets[:10])))\n",
        "ax[0].set_yticklabels(pd.Series(all_targets).value_counts().index.tolist()[:10])\n",
        "ax[0].set_title('All Targets')\n",
        "ax[1].barh(np.arange(len(all_precursors[:10])), pd.Series(all_precursors).value_counts().values.tolist()[:10])\n",
        "ax[1].invert_yaxis()\n",
        "ax[1].set_yticks(np.arange(len(all_precursors[:10])))\n",
        "ax[1].set_yticklabels(pd.Series(all_precursors).value_counts().index.tolist()[:10])\n",
        "ax[1].set_title('All Precursors')\n",
        "ax[2].barh(np.arange(len(all_operations[:10])), pd.Series(all_operations).value_counts().values.tolist()[:10])\n",
        "ax[2].invert_yaxis()\n",
        "ax[2].set_yticks(np.arange(len(all_operations[:10])))\n",
        "ax[2].set_yticklabels(pd.Series(all_operations).value_counts().index.tolist()[:10])\n",
        "ax[2].set_title('All Operations')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2i6LMo1U19Ut"
      },
      "source": [
        "Next, we will associate precursors and operations with specific targets. This is trickier than it may appear and requires us to make assumptions about the associations between targets, precursors, and operations within a paper. To keep things simple for the tutorial, we assume any precursor or operation within the same paper as our specified target is being used in the synthesis of that target. For the tutorial, we are only going to look at a couple of the more common targets we found above: Carbon Nanotubes (CNT), Bi2Te3, and Tetraphenylporphyrin (TPP). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eik59Gaf9-XW"
      },
      "source": [
        "targets_dict = {}\n",
        "curr_index = 0\n",
        "targets = ['CNT', 'Bi2Te3', 'TPP']\n",
        "for t in targets:\n",
        "  targets_dict[t] = {}\n",
        "  targets_dict[t]['operations'], targets_dict[t]['precursors'], targets_dict[t]['names'] = [],[],[]\n",
        "for paper in data['data']:\n",
        "  if paper['split'] == 'test':\n",
        "    curr_targets, curr_precursors, curr_operations = [],[],[]\n",
        "    for tokens in paper['tokens'][1:]:\n",
        "      prev_label = ''\n",
        "      for token, label in zip(tokens, test_predictions[curr_index]):\n",
        "        if label == 'target':\n",
        "          if prev_label == 'target':\n",
        "            curr_targets[-1] = curr_targets[-1]+' '+token\n",
        "          else:\n",
        "            curr_targets.append(token)\n",
        "        elif label == 'precursor':\n",
        "          if prev_label == 'precursor':\n",
        "            curr_precursors[-1] = curr_precursors[-1]+' '+token\n",
        "          else:\n",
        "            curr_precursors.append(token)\n",
        "        elif label == 'operation':\n",
        "          if prev_label == 'operation':\n",
        "            curr_operations[-1] = curr_operations[-1]+' '+token\n",
        "          else:\n",
        "            curr_operations.append(token)\n",
        "      curr_index+=1\n",
        "    for t in targets:\n",
        "      for c in curr_targets:\n",
        "        if t in c:\n",
        "          targets_dict[t]['names'].append(c)\n",
        "          targets_dict[t]['precursors'].extend(curr_precursors)\n",
        "          targets_dict[t]['operations'].extend(curr_operations)\n",
        "for t in targets:\n",
        "  targets_dict[t]['names'] = list(np.unique(targets_dict[t]['names']))\n",
        "  targets_dict[t]['precursors'] = list(np.unique(targets_dict[t]['precursors']))\n",
        "  targets_dict[t]['operations'] = list(np.unique(targets_dict[t]['operations']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1ObOUOD6DE0"
      },
      "source": [
        "Now, we can examine the precursors and operations that are used for each target. The \"names\" field keeps track of all the different target variants that were found. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9CgUBj09-1V"
      },
      "source": [
        "for t in targets_dict:\n",
        "  print(t)\n",
        "  print('   Names:', [t for t in targets_dict[t]['names'] if any(c.isalpha() for c in t)])\n",
        "  print('   Precursors:', [t for t in targets_dict[t]['precursors'] if any(c.isalpha() for c in t)])\n",
        "  print('   Operations:', [t for t in targets_dict[t]['operations'] if any(c.isalpha() for c in t)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "If8vFmX36c9Q"
      },
      "source": [
        "## Temperature Text Mining\n",
        "For the second data mining activity, we will extract synthesis temperatures and build in levels of detail around the temperatures. \n",
        "\n",
        "First we will just extract all the temperatures from the data. This can be done relatively easily by taking the words before the token \"degC\". This should give us a relatively accurate and precise set of temperatures."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGWz3pHCdgeS"
      },
      "source": [
        "all_temperatures = []\n",
        "for paper in data['data']:\n",
        "  if paper['split'] == 'test':\n",
        "    for tokens in paper['tokens'][1:]:\n",
        "      prev_token = ''\n",
        "      for token in tokens:\n",
        "        if token == 'degC':\n",
        "          try:\n",
        "            all_temperatures.append(float(prev_token))\n",
        "          except:\n",
        "            pass\n",
        "        prev_token = token\n",
        "print('Number of Temperatures Found:', len(all_temperatures))\n",
        "print('Minimum Temperature:', np.min(all_temperatures))\n",
        "print('Maximum Temperature:', np.max(all_temperatures))\n",
        "print('Mean Temperature:', round(np.mean(all_temperatures),1))\n",
        "print('Median Temperature:', np.median(all_temperatures))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFNSdrvC7lMm"
      },
      "source": [
        "We visualize the temperature distributions using a violin plot which is a nice visualization choice for adding additional levels of detail. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwnTM7Jrdgx_"
      },
      "source": [
        "temp_data = pd.DataFrame({'Temperature':all_temperatures})\n",
        "sns.violinplot(data=temp_data, y='Temperature')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9LjImek8Pz8"
      },
      "source": [
        "Next, we want to add levels of nuance to the temperature data. We will split the temperatures based on which operations, or synthesis step, it occurs in. To determine that, we will take whichever operation is closest to the temperature within the sentence.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwt1c_A8g2Qh"
      },
      "source": [
        "temperatures, operations = [],[]\n",
        "label_index_count = 0\n",
        "for paper in data['data']:\n",
        "  if paper['split'] == 'test':\n",
        "    for tokens in paper['tokens'][1:]:\n",
        "      prev_token = ''\n",
        "      for token in tokens:\n",
        "        if token == 'degC':\n",
        "          try:\n",
        "            curr_temp = float(prev_token)\n",
        "            temp_index = tokens.index(prev_token)\n",
        "            curr_operations, curr_operation_indexes = [],[]\n",
        "            for i, (token, label) in enumerate(zip(tokens, test_predictions[label_index_count])):\n",
        "              if label == 'operation':\n",
        "                curr_operations.append(token)\n",
        "                curr_operation_indexes.append(i)\n",
        "            if len(curr_operation_indexes) > 0:\n",
        "              closest_index = np.argmin([abs(temp_index-c) for c in curr_operation_indexes])\n",
        "              temperatures.append(curr_temp)\n",
        "              operations.append(curr_operations[closest_index])\n",
        "          except:\n",
        "            pass\n",
        "        prev_token = token\n",
        "      label_index_count+=1\n",
        "print('Number of Temperatures and Operations:', len(temperatures), len(operations))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eTjZrjm9uV8"
      },
      "source": [
        "To make the visualization cleaner, we only take the top five most common operations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7ipRmRPjcuZ"
      },
      "source": [
        "most_common_ops = pd.Series(operations).value_counts().index.tolist()[:5]\n",
        "common_temps, common_ops = [],[]\n",
        "for t, o in zip(temperatures, operations):\n",
        "  if o in most_common_ops:\n",
        "    common_temps.append(t)\n",
        "    common_ops.append(o)\n",
        "print('Number of Temperatures from top five operations:', len(common_temps), len(common_ops))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ezu7Gue9-6S"
      },
      "source": [
        "We plot the temperature data split by operations. We see calcination has a much higher spread in temperatures than the rest of the operations, while operations like drying and stirring have a much more consistent temperature across the data set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keHGl8-5jBgy"
      },
      "source": [
        "temp_op_data = pd.DataFrame({'Temperature':common_temps, 'Operation':common_ops})\n",
        "sns.violinplot(data=temp_op_data, y='Temperature', x='Operation')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DbJMzAJ-avc"
      },
      "source": [
        "Finally, we add a target dimension to the temperature extraction. We follow the same assumption as before that all operations and temperatures occuring in the same paper as the target are being done to make the target. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMQV44-bkBm2"
      },
      "source": [
        "temperatures, operations, targets = [],[],[]\n",
        "label_index_count = 0\n",
        "curr_index = 0\n",
        "for paper in data['data']:\n",
        "  if paper['split'] == 'test':\n",
        "    curr_targets = []\n",
        "    for tokens in paper['tokens'][1:]:\n",
        "      prev_label = ''\n",
        "      for token, label in zip(tokens, test_predictions[curr_index]):\n",
        "        if label == 'target':\n",
        "          if prev_label == 'target':\n",
        "            curr_targets[-1] = curr_targets[-1]+' '+token\n",
        "          else:\n",
        "            curr_targets.append(token)\n",
        "      curr_index+=1\n",
        "    if len(curr_targets) == 0:\n",
        "      label_index_count = label_index_count + len(paper['tokens'][1:])\n",
        "      continue\n",
        "    for tokens in paper['tokens'][1:]:\n",
        "      prev_token = ''\n",
        "      for token in tokens:\n",
        "        if token == 'degC':\n",
        "          try:\n",
        "            curr_temp = float(prev_token)\n",
        "            temp_index = tokens.index(prev_token)\n",
        "            curr_operations, curr_operation_indexes = [],[]\n",
        "            for i, (token, label) in enumerate(zip(tokens, test_predictions[label_index_count])):\n",
        "              if label == 'operation':\n",
        "                curr_operations.append(token)\n",
        "                curr_operation_indexes.append(i)\n",
        "            if len(curr_operation_indexes) > 0:\n",
        "              closest_index = np.argmin([abs(temp_index-c) for c in curr_operation_indexes])\n",
        "              for c in curr_targets:\n",
        "                if len(c) > 1:\n",
        "                  temperatures.append(curr_temp)\n",
        "                  operations.append(curr_operations[closest_index])\n",
        "                  targets.append(c)\n",
        "          except:\n",
        "            pass\n",
        "        prev_token = token\n",
        "      label_index_count+=1\n",
        "print('Number of temperatures, operations, and targets:', len(temperatures), len(operations), len(targets))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3mZ00lN-wMD"
      },
      "source": [
        "We clean the data first by filtering down to only two targets, nickel oxides and carbon nanotubes. We then combine similar operations to give us more data to work with. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhrDo-z8ogZL"
      },
      "source": [
        "small_temperatures, small_operations, small_targets = [],[],[]\n",
        "for t, o, targ in zip(temperatures, operations, targets):\n",
        "  if 'NiO' in targ:\n",
        "    small_temperatures.append(t)\n",
        "    small_operations.append(o)\n",
        "    small_targets.append('NiO')\n",
        "  elif 'CNT' in targ:\n",
        "    small_temperatures.append(t)\n",
        "    small_operations.append(o)\n",
        "    small_targets.append('CNT')\n",
        "cleaned_small_temperatures, cleaned_small_operations, cleaned_small_targets = [],[],[]\n",
        "for t, o, targ in zip(small_temperatures, small_operations, small_targets):\n",
        "  if o == 'dried' or o == 'drying':\n",
        "    cleaned_small_temperatures.append(t)\n",
        "    cleaned_small_operations.append('dry')\n",
        "    cleaned_small_targets.append(targ)\n",
        "  elif o == 'calcined' or o == 'calcination':\n",
        "    cleaned_small_temperatures.append(t)\n",
        "    cleaned_small_operations.append('calcine')\n",
        "    cleaned_small_targets.append(targ)\n",
        "  elif o == 'heated' or o == 'held' or o == 'set' or o == 'crystallization':\n",
        "    cleaned_small_temperatures.append(t)\n",
        "    cleaned_small_operations.append('heat')\n",
        "    cleaned_small_targets.append(targ)\n",
        "  elif o == 'stirred':\n",
        "    cleaned_small_temperatures.append(t)\n",
        "    cleaned_small_operations.append('stir')\n",
        "    cleaned_small_targets.append(targ)\n",
        "print('Final cleaned numbers of temperatures, operations, and targets:', len(cleaned_small_temperatures), len(cleaned_small_operations), len(cleaned_small_targets))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_KQklDiAH0b"
      },
      "source": [
        "The final visualization shows the temperature data split by operation and targets. We see that nickel oxides have a wide range of calcination temperatures whereas carbon nanotubes seem to have only a single calcination temperature. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5P47rMqkB_H"
      },
      "source": [
        "temp_op_data = pd.DataFrame({'Temperature':cleaned_small_temperatures, 'Operation':cleaned_small_operations, 'Target':cleaned_small_targets})\n",
        "sns.violinplot(data=temp_op_data, y='Temperature', x='Operation', hue='Target', split=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4psTBShuDe6G"
      },
      "source": [
        "# End of tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7AQhSqgubqI"
      },
      "source": [
        "# Additional Demonstrations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggatVuK9u1Uj"
      },
      "source": [
        "## Setup Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuTLseqZtO-6"
      },
      "source": [
        "We need to download and install the bilm library from Github. To have the code work on CoLab, we need to first run the two cells below, then restart the runtime instance so it is recognized as an installed package. \n",
        "https://stackoverflow.com/questions/57838013/modulenotfounderror-after-successful-pip-install-in-google-colaboratory\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3aXjL_qkBBB"
      },
      "source": [
        "!git clone https://github.com/allenai/bilm-tf.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "908gOtEWjAui"
      },
      "source": [
        "%cd bilm-tf/\n",
        "!python setup.py install"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfYNhQuML4Nn"
      },
      "source": [
        "******Restart the runtime*******"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8pCTHH8LUbt"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import json\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import logging\n",
        "logging.getLogger('tensorflow').disabled = True #OPTIONAL - to disable outputs from Tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XP1xexSaLVFs"
      },
      "source": [
        "!git clone https://github.com/olivettigroup/materials-synthesis-generative-models.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXJ0yBX7Lb7k"
      },
      "source": [
        "%cd materials-synthesis-generative-models/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EYckiL_LcU8"
      },
      "source": [
        "!wget https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/vocab-2016-09-10.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHadDwH2Lcb0"
      },
      "source": [
        "# Download the pretrained-Elmo weights and config file from https://figshare.com/s/ec677e7db3cf2b7db4bf\n",
        "!wget https://ndownloader.figshare.com/files/13773791?private_link=ec677e7db3cf2b7db4bf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahK1of_bLcsN"
      },
      "source": [
        "# Unzip the weights and config file\n",
        "!tar -xvf '13773791?private_link=ec677e7db3cf2b7db4bf'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2xbnm3xLlsX"
      },
      "source": [
        "from models import token_classifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giEvzmzTLl58"
      },
      "source": [
        "token_classifier = token_classifier.TokenClassifier(\n",
        "    vocab=\"vocab-2016-09-10.txt\", \n",
        "    options=\"elmo_options.json\", \n",
        "    weights=\"elmo_weights.hdf5\",\n",
        "    use_cpu=True, load_data=False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e14ixtXWQny"
      },
      "source": [
        "\n",
        "## Featurize Data and Train NER Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dovLtqfbvrm5"
      },
      "source": [
        "Load in the data from the repository "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ss9DsKsfWTXe"
      },
      "source": [
        "with open('data/ner_annotations_split.json', 'r') as f:\n",
        "  data = json.load(f)\n",
        "print(data.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocmZUxBYXHEe"
      },
      "source": [
        "train_sentences, dev_sentences, test_sentences = [],[],[]\n",
        "train_labels, dev_labels, test_labels = [],[],[]\n",
        "for paper in data['data']:\n",
        "  if paper['split'] == 'train':\n",
        "    train_sentences.extend(paper['tokens'][1:]) # first \"sentence\" is the title which we don't want right now\n",
        "    train_labels.extend(paper['labels'][1:])\n",
        "  elif paper['split'] == 'dev':\n",
        "    dev_sentences.extend(paper['tokens'][1:])\n",
        "    dev_labels.extend(paper['labels'][1:])\n",
        "  else:\n",
        "    test_sentences.extend(paper['tokens'][1:])\n",
        "    test_labels.extend(paper['labels'][1:])\n",
        "print(len(train_sentences), len(dev_sentences), len(test_sentences))\n",
        "print(len(train_labels), len(dev_labels), len(test_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUGtBT71vT4a"
      },
      "source": [
        "Featurize the sentences into arrays of ELMO embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfnW9SVhbBrc"
      },
      "source": [
        "train_elmo_features = token_classifier.featurize_elmo_list(train_sentences)\n",
        "print('Train Input shape:', train_elmo_features.shape)\n",
        "dev_elmo_features = token_classifier.featurize_elmo_list(dev_sentences)\n",
        "print('Dev Input shape:', dev_elmo_features.shape)\n",
        "test_elmo_features = token_classifier.featurize_elmo_list(test_sentences)\n",
        "print('Test Input shape:', test_elmo_features.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHn-aGv1vYw6"
      },
      "source": [
        "One-hot encode the annotation labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7YNTd9LdWVx"
      },
      "source": [
        "y_train, y_dev, y_test = [],[],[]\n",
        "for labels in train_labels:\n",
        "  train_onehot_labels = np.zeros(shape=(token_classifier._seq_maxlen, len(token_classifier.token_classes)))\n",
        "  for j, label in enumerate(labels[:token_classifier._seq_maxlen]):\n",
        "    if label not in ['precursor', 'target', 'operation']:\n",
        "      label = 'null'\n",
        "    train_onehot_label = [0.0]*len(token_classifier.token_classes)\n",
        "    train_onehot_label[token_classifier.inv_token_classes[label]] = 1.0\n",
        "    train_onehot_labels[j] = train_onehot_label\n",
        "  y_train.append(train_onehot_labels)\n",
        "for labels in dev_labels:\n",
        "  dev_onehot_labels = np.zeros(shape=(token_classifier._seq_maxlen, len(token_classifier.token_classes)))\n",
        "  for j, label in enumerate(labels[:token_classifier._seq_maxlen]):\n",
        "    if label not in ['precursor', 'target', 'operation']:\n",
        "        label = 'null'\n",
        "    dev_onehot_label = [0.0]*len(token_classifier.token_classes)\n",
        "    dev_onehot_label[token_classifier.inv_token_classes[label]] = 1.0\n",
        "    dev_onehot_labels[j] = dev_onehot_label\n",
        "  y_dev.append(dev_onehot_labels)\n",
        "for labels in test_labels:\n",
        "  test_onehot_labels = np.zeros(shape=(token_classifier._seq_maxlen, len(token_classifier.token_classes))) \n",
        "  for j, label in enumerate(labels[:token_classifier._seq_maxlen]):\n",
        "    if label not in ['precursor', 'target', 'operation']:\n",
        "        label = 'null'\n",
        "    test_onehot_label = [0.0]*len(token_classifier.token_classes)\n",
        "    test_onehot_label[token_classifier.inv_token_classes[label]] = 1.0\n",
        "    test_onehot_labels[j] = test_onehot_label\n",
        "  y_test.append(test_onehot_labels)\n",
        "y_test = np.array(y_test)\n",
        "y_dev = np.array(y_dev)\n",
        "y_train = np.array(y_train)\n",
        "print('Train Output Shape:', y_train.shape)\n",
        "print('Dev Output Shape:', y_dev.shape)\n",
        "print('Test Output Shape:', y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XPUpA8_vXwH"
      },
      "source": [
        "Set variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71E7JUgVkSiE"
      },
      "source": [
        "token_classifier.X_train = train_elmo_features\n",
        "token_classifier.X_dev = dev_elmo_features\n",
        "token_classifier.X_test = test_elmo_features\n",
        "token_classifier.Y_train = y_train\n",
        "token_classifier.Y_dev = y_dev\n",
        "token_classifier.Y_test = y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iGXE31YvhDI"
      },
      "source": [
        "Build the model in Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTP46V0FuEMe"
      },
      "source": [
        "token_classifier.build_nn_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efJ7d20cvjbb"
      },
      "source": [
        "Train the model using early stopping to prevent overfitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QczeDWRLgeGQ"
      },
      "source": [
        "token_classifier.train(stop_early=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjS4sT5qvn1c"
      },
      "source": [
        "Save model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGXGthJ-g0FN"
      },
      "source": [
        "token_classifier.save(\"bin/model_name.model\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}